input {
  s3 {
    bucket => "${BUCKET_NAME}"
    role_arn => "${ROLE_ARN}"
    region => "${REGION}"
    codec => "json"
  }
}

filter {
  if [message] =~ /Logstash permission check.*/ { drop { } }
  if [type] == "forti_authenticator_log" {
    grok {
      match => ["message", "%{SYSLOG5424PRI:syslog_index}%{GREEDYDATA:message}"]
      overwrite => [ "message" ]
      tag_on_failure => [ "forti_grok_failure" ]
    }


    kv {
      source => "message"
      value_split => "="
      #Expects you have csv enable set on your Fortigate. If not I think you'll have to change it to " " but I didn't test that.
      field_split => " "
    }

    mutate {
      #I want to use the timestamp inside the logs instead of Logstash's timestamp so we'll first create a new field containing the date and time fields from the syslog before we convert that to the @timestamp field
      add_field => { "temp_time" => "%{date} %{time}" }
      #The syslog contains a type field which messes with the Logstash type field so we have to rename it.
      rename => { "type" => "ftg_type" }
      rename => { "subtype" => "ftg_subtype" }
      add_field => { "type" => "forti_log" }
      convert => { "rcvdbyte" => "integer" }
      convert => { "sentbyte" => "integer" }
    }

    date {
      match => [ "temp_time", "yyyy-MM-dd HH:mm:ss" ]
      timezone => "UTC"
      target => "@timestamp"
    }

    mutate {
      #add/remove fields as you see fit.
      remove_field => ["syslog_index","syslog5424_pri","path","temp_time","service","sentpkt","rcvdpkt","log_id","message","poluuid"]
    }
  }
}

output {
  if [bc_messageType] in ["win_security", "forti_authenticator_log", "safenet", "oracle_alert_log", "secure_log", "aide_log"] {
    amazon_es {
      hosts => ["${ES_URL}"]
      region => "us-west-2"
      index => "%{bc_customerName}_%{bc_messageType}_index-%{+YYYY-MM-dd}"
    }
  }
  else {
    stdout {codec => rubydebug}
  }
}
