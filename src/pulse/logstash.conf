## Collect transaction data over TCP - no filters necessary
input {
  tcp {
    port = > 5000
    codec = > json_lines
    add_field = > {
      "type" = > "pulse_transactions"
    }
  }
}

## Collect log entries over HTTP, these are parsed using filters
input {
  http {
    port = > 8080
    codec = > json_lines
    add_field = > {
      "type" = > "pulse_logs"
    }
  }
}


filter {
  if [type] == "pulse_logs" {
    if [sourcetype] == "zxtm_event_log" or [sourcetype] == "zxtm_dpa_log" {
      # Parse the basic structure of the event log
      grok {
        match = > {
          "event" = >
          "\[[^\]]+\]\s+(?<severity>[^\t:]+)\t(?<event_tags>.*\t)?(?<message>[^\t]*)"
        }
      }
      # Extract configuration objects from the set of tags
      kv {
        source = > "event_tags"
        target = > "objects"
        recursive = > "true"
        field_split = > "\t"
        value_split = > "\/"
        trim_value = > "\t"
      }
      # Split the rest of the tags into an array
      mutate {
        gsub = > ["event_tags", "[a-zA-Z0-9_]+\/.+?\t", ""]
        gsub = > ["event_tags", "\t", " "]
        split = > {"event_tags" = > " "}
      }
    } else if [sourcetype] == "zxtm_audit_log" {
      # Parse the basic structure of the audit log
      grok {
        match = > {"event" = > "\[(?<timestamp>.*?)\]\t(?<attributes>.*)"}
      }

      # Separate out the attributes into key/value pairs
      kv {
        source = > "attributes"
        field_split = > "\t"
        value_split = > "="
        trim_key = > "\t"
        trim_value = > "\t"
        remove_field = > ["attributes"]
      }
      # Parse the date (as it is not included in records exported
      # by version 17.2 of the traffic manager)
      date {
        locale = > "en"
        match = > ["timestamp", "dd/MMM/yyyy:HH:mm:ss Z"]
        target = > "@timestamp"
        remove_field = > ["timestamp"]
      }

    } else if [sourcetype] == "syslog" {
      # Use built-in syslog format parser, but allow both classic and
      # modern variation on the timestamp
      grok {
        match = > {
          "event" = > [
            "%{SYSLOGBASE}",
            "%{TIMESTAMP_ISO8601:timestamp} (?:%{SYSLOGFACILITY}
            )?%{SYSLOGHOST: logsource} %{SYSLOGPROG}:"
          ]
        }
      }

      date {
        locale = > "en"
        match = > ["timestamp", "MMM dd HH:mm:ss", "MMM d HH:mm:ss", "ISO8601"]
        target = > "@timestamp"
        remove_field = > ["timestamp"]
      }
    } else if [sourcetype] == "zxtm_waf_log" {
      grok {
        match = > {
          "event" = > "(?<timestamp>\d+-\d+-
          \d+\s+\d+: \d+:\d+, \d+)\s+(?<process>[^\s]*)\s+(?<severity>\S+)\s+(?<message>.*)" }
        }
        date {
          locale = > "en"
          match = > ["timestamp", "yyyy-MM-dd HH:mm:ss,SSS"]
          timezone = > "UTC" # **SET_TIMEZONE** Set your own timezone
          target = > "@timestamp"
          remove_field = > ["timestamp"]
        }
      }
      # If the exported data already contained a parsed timestamp,
      # use that as the entry's official timestamp.
      date {
        locale = > "en"
        match = > ["time", "UNIX"]
        target = > "@timestamp"
        remove_field = > ["time"]
      }
    }
  }
}

## Output the record to an appropriate index in Elastic Search
output {
  stdout {codec = > json}
  http {
    url = > "${BC_URL}"
    http_method = > "post"
    format = > "json"
  }
}
